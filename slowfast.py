# -*- coding: utf-8 -*-
"""Video_processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14uYZPDJFm2h0WpSort-56CvkbH8Dlc2w
"""

!pip install decord

import os
import time
import cv2
import torch
import numpy as np
from decord import VideoReader, cpu
from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score, confusion_matrix, classification_report, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Set device
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

CLIP_LENGTH = 32
FRAME_SIZE = 256

# Download Kinetics-400 label map file (only run once per session)
import urllib.request
LABELS_URL = "https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt"
urllib.request.urlretrieve(LABELS_URL, "kinetics_labels.txt")

# Load Kinetics-400 labels
with open("kinetics_labels.txt", "r") as f:
    kinetics_labels = [line.strip() for line in f.readlines()]

print(f"Total Kinetics-400 labels: {len(kinetics_labels)}")
print("Sample labels:", kinetics_labels[:10])  # Show first 10 labels

# Define keywords for "punch" group
punch_keywords = ["punch", "boxing"]

# Group labels into punch and non-punch
punch_labels = []
non_punch_labels = []

for label in kinetics_labels:
    label_lower = label.lower()
    if any(keyword in label_lower for keyword in punch_keywords):
        punch_labels.append(label)
    else:
        non_punch_labels.append(label)

# Display grouping results
print("\n--- Punch-related classes ---")
for pl in punch_labels:
    print(pl)
print(f"\nTotal punch-related classes: {len(punch_labels)}")

print("\n--- Non-punch classes (rest of Kinetics labels) ---")
print(f"Total non-punch classes: {len(non_punch_labels)}")

# Optional: show first 10 non-punch classes
print("\nSample non-punch classes:", non_punch_labels[:10])

# Load Kinetics-400 labels
with open("kinetics_labels.txt", "r") as f:
    KINETICS_LABELS = [line.strip().lower() for line in f.readlines()]

def evaluate_model_action_data(data_dir, action_model):
    classes = ['non_punch', 'punch']
    label_to_idx = {cls: idx for idx, cls in enumerate(classes)}

    preds = []
    true_labels = []
    confidences = []
    total_frames = 0
    start_time = time.time()

    for cls in classes:
        cls_dir = os.path.join(data_dir, cls)
        if not os.path.exists(cls_dir):
            print(f"Warning: {cls_dir} not found. Skipping this class.")
            continue
        video_files = [os.path.join(cls_dir, f) for f in sorted(os.listdir(cls_dir)) if f.endswith(('.avi', '.mp4', '.mov', '.mkv'))]
        print(f"Found {len(video_files)} videos in {cls_dir}")

        for video_path in video_files:
            try:
                vr = VideoReader(video_path, ctx=cpu(0))
                frame_buffer = []
                for i in range(len(vr)):
                    frame = vr[i].asnumpy()
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    frame_buffer.append(frame)
                    if len(frame_buffer) == CLIP_LENGTH:
                        resized = [cv2.resize(f, (FRAME_SIZE, FRAME_SIZE)) for f in frame_buffer]
                        frames = np.array(resized).transpose(3, 0, 1, 2) / 255.0
                        frames = torch.tensor(frames, dtype=torch.float32).unsqueeze(0).to(DEVICE)
                        slow_frames = frames[:, :, ::4, :, :]
                        inputs = [slow_frames, frames]
                        with torch.no_grad():
                            preds_logits = action_model(inputs)
                            conf_softmax = preds_logits.softmax(dim=1)
                            conf, pred_class = torch.max(conf_softmax, 1)
                        pred_label_str = KINETICS_LABELS[pred_class.item()]
                        is_punch = 1 if ("punch" in pred_label_str or "boxing" in pred_label_str) else 0
                        preds.append(is_punch)
                        true_labels.append(label_to_idx[cls])
                        confidences.append(conf.item())
                        frame_buffer = []
                total_frames += len(vr)
            except Exception as e:
                print(f"Error processing {video_path}: {str(e)}")
                continue

    total_time = time.time() - start_time
    return {
        "true_labels": np.array(true_labels),
        "preds": np.array(preds),
        "confidences": np.array(confidences),
        "total_frames": total_frames,
        "total_time": total_time,
    }

!pip install pytorchvideo

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

from pytorchvideo.models.hub import slowfast_r50
action_model = slowfast_r50(pretrained=True).to(DEVICE).eval()

test_dir = "/content/drive/MyDrive/action_data"   # <--- YOUR DATASET PATH HERE
results = evaluate_model_action_data(test_dir, action_model)

# Compute metrics and plot (identical to previous)
true_labels = results["true_labels"]
preds = results["preds"]
confidences = results["confidences"]

accuracy = (preds == true_labels).mean()
precision = precision_score(true_labels, preds, average="binary")
recall = recall_score(true_labels, preds, average="binary")
f1 = f1_score(true_labels, preds, average="binary")
ap = average_precision_score(true_labels, confidences)
cm = confusion_matrix(true_labels, preds)
report = classification_report(true_labels, preds, target_names=["non_punch", "punch"])

print("\nModel Evaluation Metrics")
print("-"*40)
print(f"Accuracy           : {accuracy:.4f}")
print(f"Precision          : {precision:.4f}")
print(f"Recall             : {recall:.4f}")
print(f"F1 Score           : {f1:.4f}")
print(f"Average Precision  : {ap:.4f}")
print("Confusion Matrix:\n", cm)
print("Classification Report:\n", report)

ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['non_punch', 'punch']).plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import PrecisionRecallDisplay
PrecisionRecallDisplay.from_predictions(true_labels, confidences)
plt.title("Precision-Recall Curve")
plt.show()

"""Version - 2"""

# Install if needed (uncomment to run)
!pip install pytorchvideo torchvision decord

!pip install torch==2.0.1 torchvision==0.15.2 --extra-index-url https://download.pytorch.org/whl/cu118
!pip install git+https://github.com/facebookresearch/pytorchvideo

# Change import in pytorchvideo source from functional_tensor to functional
!sed -i 's/import torchvision.transforms.functional_tensor as F_t/import torchvision.transforms.functional as F_t/' $(python -c "import os,pytorchvideo; print(os.path.dirname(pytorchvideo.__file__))")/transforms/augmentations.py

import torch
from torchvision.transforms import Compose, Lambda, CenterCrop, Normalize
from pytorchvideo.transforms import ApplyTransformToKey, UniformTemporalSubsample, ShortSideScale
from torchvision.transforms import Compose, Lambda
from pytorchvideo.models.hub import slowfast_r50
import urllib.request

# Device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Download Kinetics-400 labels (if not already downloaded)
# LABELS_URL = "https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt"
# urllib.request.urlretrieve(LABELS_URL, "kinetics_labels.txt")

from pytorchvideo.transforms import Normalize, UniformTemporalSubsample, ShortSideScale, ApplyTransformToKey
from torchvision.transforms import Compose, Lambda, CenterCrop

with open("kinetics_labels.txt", "r") as f:
    KINETICS_LABELS = [line.strip() for line in f.readlines()]

print(f"Loaded {len(KINETICS_LABELS)} Kinetics-400 labels")

# Define preprocessing transforms for SlowFast
transform = ApplyTransformToKey(
    key="video",
    transform=Compose([
        UniformTemporalSubsample(32),   # sample 32 frames
        Lambda(lambda x: x / 255.0),    # scale pixels to [0,1]
        Normalize(                     # normalize channels with given mean/std
            mean=[0.45, 0.45, 0.45],
            std=[0.225, 0.225, 0.225],
        ),
        ShortSideScale(256),
        CenterCrop(224)
    ])
)

# Load pretrained SlowFast model
model = slowfast_r50(pretrained=True)
model = model.to(device)
model.eval()

# Function to preprocess video and predict
def predict_video(video_path):
    video = EncodedVideo.from_path(video_path)
    start_sec, end_sec = 0, 3  # use first 3 seconds clip; adjust as needed
    clip = video.get_clip(start_sec=start_sec, end_sec=end_sec)
    clip = transform(clip)
    inputs = [clip["video"].to(device)[None, ...]]  # create batch dim

    with torch.no_grad():
        preds = model(inputs)
        preds = torch.nn.functional.softmax(preds, dim=1)
        conf, pred_class = torch.max(preds, 1)
        label = KINETICS_LABELS[pred_class]

    print(f"Predicted Action: {label} (Confidence: {conf.item():.3f})")

    # Binary grouping
    label_str = label.lower()
    if "punch" in label_str or "boxing" in label_str:
        print("Classified as: PUNCH")
    else:
        print("Classified as: NON-PUNCH")

    return label.item(), conf.item()

# Example usage
video_path = "/content/drive/MyDrive/punch/test/videos/v_Punch_g01_c04.avi"  # replace with your video path
predict_video(video_path)

