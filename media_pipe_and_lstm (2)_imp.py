# -*- coding: utf-8 -*-
"""Media_Pipe_And_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PMtt1zWVj046IK6_dJ1yWPuIguvllGst

Fixed dependency versions to resolve compatibility issues

Ensured compatibility between MediaPipe (requires protobuf<5) and TensorFlow 2.12
"""

!pip install numpy==1.24.3 protobuf==3.20.3

!pip install mediapipe tensorflow opencv-python

# [2] Imports
import cv2
import numpy as np
import os
import mediapipe as mp
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.model_selection import train_test_split

"""Step 2: Landmark Extraction
Processes each video frame to get 3D coordinates of body and hand joints

Input: RGB frame → Output: 225-dimensional vector per frame
"""

# [3] Initialize MediaPipe Holistic
mp_holistic = mp.solutions.holistic
holistic = mp_holistic.Holistic()

# [4] Extract landmarks function
def extract_landmarks(frame):
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = holistic.process(frame_rgb)

    # Extract pose, left hand, right hand landmarks
    pose = np.array([[lmk.x, lmk.y, lmk.z]
                    for lmk in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)
    lh = np.array([[lmk.x, lmk.y, lmk.z]
                  for lmk in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[lmk.x, lmk.y, lmk.z]
                  for lmk in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)

    return np.concatenate([pose, lh, rh])

# [5] Parameters
SEQUENCE_LENGTH = 30  # Number of frames per sample
NUM_FEATURES = (33+21+21)*3  # 75 landmarks * 3 (x,y,z)
ACTIONS = ['punch', 'non_punch']  # Your classes

"""Step 4: Data Preparation

Organized videos into punch and non_punch folders

Generated 30-frame sliding window samples
"""

# [6] Create dataset function
def create_dataset(data_dir):
    X, y = [], []

    for action in ACTIONS:
        action_dir = os.path.join(data_dir, action)
        print(f"Processing {action}...")

        for video_file in os.listdir(action_dir):
            video_path = os.path.join(action_dir, video_file)
            cap = cv2.VideoCapture(video_path)
            sequence = []

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret: break

                landmarks = extract_landmarks(frame)
                sequence.append(landmarks)

                if len(sequence) == SEQUENCE_LENGTH:
                    X.append(sequence)
                    y.append(ACTIONS.index(action))
                    sequence = []

            cap.release()

    return np.array(X), np.array(y)

"""Step 3: Model Architecture

Input: Sequences of 30 frames (225 features each)

Output: Probability distribution over 2 classes (punch/non_punch)

Uses two LSTM layers to capture temporal patterns
"""

# [7] Build LSTM model
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(SEQUENCE_LENGTH, NUM_FEATURES)),
    LSTM(128),
    Dense(64, activation='relu'),
    Dense(len(ACTIONS), activation='softmax')
])
model.compile(loss='sparse_categorical_crossentropy',
             optimizer='adam',
             metrics=['accuracy'])

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""Step 5: Training

Train/Test Split: 80/20


"""

# [8] Train model (example - replace with your data)

# Create dataset (replace with your directory structure)
# Dataset should have:
# /content/drive/MyDrive/action_data/
#   ├── punch/
#   │    └── video1.avi, video2.avi...
#   └── non_punch/
#        └── video1.avi, video2.avi...
X, y = create_dataset('/content/drive/MyDrive/action_data')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))

# [9] Save model
model.save('/content/drive/MyDrive/action_recognition.h5')
np.save('/content/drive/MyDrive/X_test.npy', X_test)
np.save('/content/drive/MyDrive/y_test.npy', y_test)

!pip install ultralytics opencv-python

# Install dependencies
!pip install mediapipe tensorflow opencv-python

# Imports
import cv2
import numpy as np
import mediapipe as mp
from tensorflow.keras.models import load_model

# Initialize MediaPipe Holistic
mp_holistic = mp.solutions.holistic
holistic = mp_holistic.Holistic()

# Landmark extraction function (225 features)
def extract_landmarks(frame):
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = holistic.process(frame_rgb)

    pose = np.array([[lmk.x, lmk.y, lmk.z]
                    for lmk in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)
    lh = np.array([[lmk.x, lmk.y, lmk.z]
                  for lmk in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[lmk.x, lmk.y, lmk.z]
                  for lmk in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)

    return np.concatenate([pose, lh, rh]), results

# Load model (ensure it's trained on 225 features)
model = load_model('/content/drive/MyDrive/action_recognition.h5')
SEQUENCE_LENGTH = 30

# Punch detection function
def punch_detection_on_video(input_video_path, output_video_path):
    cap = cv2.VideoCapture(input_video_path)
    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps    = cap.get(cv2.CAP_PROP_FPS)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    sequence = []
    print("Processing video...")
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break

        landmarks, results = extract_landmarks(frame)
        sequence.append(landmarks)

        if len(sequence) >= SEQUENCE_LENGTH:
            sequence_window = sequence[-SEQUENCE_LENGTH:]

            # Predict
            input_seq = np.expand_dims(sequence_window, axis=0)
            punch_prob = model.predict(input_seq, verbose=0)[0][0]

            # Draw bounding box if punch detected
            if punch_prob > 0.7 and results.pose_landmarks:
                xs = [lmk.x for lmk in results.pose_landmarks.landmark]
                ys = [lmk.y for lmk in results.pose_landmarks.landmark]
                x_min, x_max = int(min(xs)*width), int(max(xs)*width)
                y_min, y_max = int(min(ys)*height), int(max(ys)*height)
                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0,0,255), 2)
                cv2.putText(frame, f"PUNCH ({punch_prob:.2f})", (x_min, y_min-10),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)

            # Write frame
            out.write(frame)

    cap.release()
    out.release()
    print(f"Output saved to {output_video_path}")

# Run detection
input_video = '/content/drive/MyDrive/videoplayback (1).mp4'
output_video = '/content/drive/MyDrive/media_pipe.mp4'
punch_detection_on_video(input_video, output_video)

# Display result
from IPython.display import HTML
from base64 import b64encode

with open(output_video, 'rb') as f:
    mp4 = f.read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(f'<video width=500 controls><source src="{data_url}" type="video/mp4"></video>')

from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score
import numpy as np

# Get model predictions (returns probabilities)
y_pred_prob = model.predict(X_test)
# For binary classification, take the probability for the 'punch' class (index 0)
# If your model output shape is (N, 2), use argmax
y_pred = np.argmax(y_pred_prob, axis=1)

from tensorflow.keras.models import load_model
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_auc_score

# Load your Keras model
model = load_model('/content/drive/MyDrive/action_recognition.h5')

# Load your test data (must be numpy arrays)
X_test = np.load('/content/drive/MyDrive/X_test.npy')
y_test = np.load('/content/drive/MyDrive/y_test.npy')

# Predict
y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)
print("Macro Precision:", precision_score(y_test, y_pred, average='macro'))
print("Micro Precision:", precision_score(y_test, y_pred, average='micro'))
print("Macro Recall:", recall_score(y_test, y_pred, average='macro'))
print("Micro Recall:", recall_score(y_test, y_pred, average='micro'))
print("Macro F1:", f1_score(y_test, y_pred, average='macro'))
print("Micro F1:", f1_score(y_test, y_pred, average='micro'))
# Metrics
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, y_pred, average='macro'))
# For binary classification (adjust index if needed)
if y_pred_prob.shape[1] == 2:
    print("ROC-AUC:", roc_auc_score(y_test, y_pred_prob[:, 1]))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# y_true: ground truth labels (e.g., y_test)
# y_pred: predicted labels (e.g., from model.predict)

# Calculate confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Visualize confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['non_punch', 'punch'])
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

"""# **Final Code**"""

# Imports
import os
import cv2
import numpy as np
import mediapipe as mp
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Set parameters
SEQUENCE_LENGTH = 30
BATCH_SIZE = 8
ACTIONS = ['punch', 'non_punch']
NUM_FEATURES = (33+21+21)*3

# Initialize MediaPipe Holistic
mp_holistic = mp.solutions.holistic
holistic = mp_holistic.Holistic()

# Landmark extraction function
def extract_landmarks(frame):
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = holistic.process(frame_rgb)
    pose = np.array([[lmk.x, lmk.y, lmk.z] for lmk in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)
    lh = np.array([[lmk.x, lmk.y, lmk.z] for lmk in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[lmk.x, lmk.y, lmk.z] for lmk in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    return np.concatenate([pose, lh, rh])

# Dataset creation function (sliding window of SEQUENCE_LENGTH)
def create_dataset(data_dir):
    X, y = [], []
    for action in ACTIONS:
        action_dir = os.path.join(data_dir, action)
        print(f"Processing {action}...")
        for video_file in os.listdir(action_dir):
            video_path = os.path.join(action_dir, video_file)
            cap = cv2.VideoCapture(video_path)
            sequence = []
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret: break
                landmarks = extract_landmarks(frame)
                sequence.append(landmarks)
                if len(sequence) == SEQUENCE_LENGTH:
                    X.append(sequence)
                    y.append(ACTIONS.index(action))
                    sequence = []
            cap.release()
    return np.array(X), np.array(y)

# Model architecture
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(SEQUENCE_LENGTH, NUM_FEATURES)),
    LSTM(128),
    Dense(64, activation='relu'),
    Dense(len(ACTIONS), activation='softmax')
])
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Mount Google Drive if needed
from google.colab import drive
drive.mount('/content/drive')

# Define dataset directory
data_dir = '/content/drive/MyDrive/action_data'

# Create dataset
X, y = create_dataset(data_dir)

# Split: 70% train, 30% temp
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
# Now split temp into 0.5/0.5 → 15%/15% of total for val and test
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

print(f"Train: {X_train.shape[0]} samples")
print(f"Validation: {X_val.shape[0]} samples")
print(f"Test: {X_test.shape[0]} samples")

# Train the model
model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=BATCH_SIZE,
    validation_data=(X_val, y_val)
)

# Save model and test splits
model.save('/content/drive/MyDrive/action_recognition.h5')
np.save('/content/drive/MyDrive/X_test.npy', X_test)
np.save('/content/drive/MyDrive/y_test.npy', y_test)

# Evaluation
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, ConfusionMatrixDisplay

# Load model and test data
model = load_model('/content/drive/MyDrive/action_recognition.h5')
X_test = np.load('/content/drive/MyDrive/X_test.npy')
y_test = np.load('/content/drive/MyDrive/y_test.npy')

# Prediction
y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)

# Metrics
print("Macro Precision:", precision_score(y_test, y_pred, average='macro'))
print("Micro Precision:", precision_score(y_test, y_pred, average='micro'))
print("Macro Recall:", recall_score(y_test, y_pred, average='macro'))
print("Micro Recall:", recall_score(y_test, y_pred, average='micro'))
print("Macro F1:", f1_score(y_test, y_pred, average='macro'))
print("Micro F1:", f1_score(y_test, y_pred, average='micro'))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
if y_pred_prob.shape[1] == 2:
    print("ROC-AUC:", roc_auc_score(y_test, y_pred_prob[:, 1]))

# Confusion matrix visualization
disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=['non_punch', 'punch'])
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

