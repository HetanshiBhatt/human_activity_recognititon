# -*- coding: utf-8 -*-
"""Video_processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14uYZPDJFm2h0WpSort-56CvkbH8Dlc2w
"""

!pip install decord pytorchvideo

import os
import time
import cv2
import torch
import numpy as np
from decord import VideoReader, cpu
from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score, confusion_matrix, classification_report, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Set device
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

CLIP_LENGTH = 32  # Sequence length updated to 30
FRAME_SIZE = 256

# Download Kinetics-400 label map file (run once per session)
import urllib.request
LABELS_URL = "https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt"
if not os.path.exists("kinetics_labels.txt"):
    urllib.request.urlretrieve(LABELS_URL, "kinetics_labels.txt")

# Load Kinetics-400 labels
with open("kinetics_labels.txt", "r") as f:
    kinetics_labels = [line.strip() for line in f.readlines()]

print(f"Total Kinetics-400 labels: {len(kinetics_labels)}")
print("Sample labels:", kinetics_labels[:10])  # Show first 10

# Define keywords for "punch" group
punch_keywords = ["punch", "boxing"]

# Group labels into punch and non-punch
punch_labels = []
non_punch_labels = []
for label in kinetics_labels:
    label_lower = label.lower()
    if any(keyword in label_lower for keyword in punch_keywords):
        punch_labels.append(label)
    else:
        non_punch_labels.append(label)

print("\n--- Punch-related classes ---")
print(punch_labels)
print(f"\nTotal punch-related classes: {len(punch_labels)}")
print(f"Total non-punch classes: {len(non_punch_labels)}")

# Load Kinetics-400 labels normalized lowercase (for prediction comparison)
with open("kinetics_labels.txt", "r") as f:
    KINETICS_LABELS = [line.strip().lower() for line in f.readlines()]

# Function to evaluate action model on the dataset folder
def evaluate_model_action_data(data_dir, action_model):
    classes = ['non_punch', 'punch']
    label_to_idx = {cls: idx for idx, cls in enumerate(classes)}

    preds = []
    true_labels = []
    confidences = []
    total_frames = 0
    start_time = time.time()

    for cls in classes:
        cls_dir = os.path.join(data_dir, cls)
        if not os.path.exists(cls_dir):
            print(f"Warning: {cls_dir} not found. Skipping this class.")
            continue
        video_files = [os.path.join(cls_dir, f) for f in sorted(os.listdir(cls_dir)) if f.endswith(('.avi', '.mp4', '.mov', '.mkv'))]
        print(f"Found {len(video_files)} videos in {cls_dir}")

        for video_path in video_files:
            try:
                vr = VideoReader(video_path, ctx=cpu(0))
                frame_buffer = []
                for i in range(len(vr)):
                    frame = vr[i].asnumpy()
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    frame_buffer.append(frame)
                    if len(frame_buffer) == CLIP_LENGTH:
                        resized = [cv2.resize(f, (FRAME_SIZE, FRAME_SIZE)) for f in frame_buffer]
                        frames = np.array(resized).transpose(3, 0, 1, 2) / 255.0
                        frames = torch.tensor(frames, dtype=torch.float32).unsqueeze(0).to(DEVICE)
                        slow_frames = frames[:, :, ::4, :, :]
                        inputs = [slow_frames, frames]

                        with torch.no_grad():
                            preds_logits = action_model(inputs)
                            conf_softmax = preds_logits.softmax(dim=1)
                            conf, pred_class = torch.max(conf_softmax, 1)

                        pred_label_str = KINETICS_LABELS[pred_class.item()]
                        is_punch = 1 if ("punch" in pred_label_str or "boxing" in pred_label_str) else 0
                        preds.append(is_punch)
                        true_labels.append(label_to_idx[cls])
                        confidences.append(conf.item())
                        frame_buffer = []
                total_frames += len(vr)
            except Exception as e:
                print(f"Error processing {video_path}: {str(e)}")
                continue

    total_time = time.time() - start_time
    return {
        "true_labels": np.array(true_labels),
        "preds": np.array(preds),
        "confidences": np.array(confidences),
        "total_frames": total_frames,
        "total_time": total_time,
    }

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

from pytorchvideo.models.hub import slowfast_r50
action_model = slowfast_r50(pretrained=True).to(DEVICE).eval()

test_dir = "/content/drive/MyDrive/action_data"   # <--- YOUR DATASET PATH HERE
results = evaluate_model_action_data(test_dir, action_model)

# Compute metrics and plot (identical to previous)
true_labels = results["true_labels"]
preds = results["preds"]
confidences = results["confidences"]

accuracy = (preds == true_labels).mean()
precision = precision_score(true_labels, preds, average="binary")
recall = recall_score(true_labels, preds, average="binary")
f1 = f1_score(true_labels, preds, average="binary")
ap = average_precision_score(true_labels, confidences)
cm = confusion_matrix(true_labels, preds)
report = classification_report(true_labels, preds, target_names=["non_punch", "punch"])

print("\nModel Evaluation Metrics")
print("-"*40)
print(f"Accuracy           : {accuracy:.4f}")
print(f"Precision          : {precision:.4f}")
print(f"Recall             : {recall:.4f}")
print(f"F1 Score           : {f1:.4f}")
print(f"Average Precision  : {ap:.4f}")
print("Confusion Matrix:\n", cm)
print("Classification Report:\n", report)

ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['non_punch', 'punch']).plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import PrecisionRecallDisplay
PrecisionRecallDisplay.from_predictions(true_labels, confidences)
plt.title("Precision-Recall Curve")
plt.show()

"""Version - 2"""

# Install if needed (uncomment to run)
!pip install pytorchvideo torchvision decord

!pip install torch==2.0.1 torchvision==0.15.2 --extra-index-url https://download.pytorch.org/whl/cu118
!pip install git+https://github.com/facebookresearch/pytorchvideo

# Change import in pytorchvideo source from functional_tensor to functional
!sed -i 's/import torchvision.transforms.functional_tensor as F_t/import torchvision.transforms.functional as F_t/' $(python -c "import os,pytorchvideo; print(os.path.dirname(pytorchvideo.__file__))")/transforms/augmentations.py

import os
import time
import torch
import numpy as np
import cv2
from decord import VideoReader, cpu
from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score, confusion_matrix, classification_report, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import urllib.request

from pytorchvideo.data.encoded_video import EncodedVideo
from pytorchvideo.transforms import ApplyTransformToKey, UniformTemporalSubsample, ShortSideScale, Normalize
from torchvision.transforms import Compose, Lambda, CenterCrop
from pytorchvideo.models.hub import slowfast_r50

# Device setup
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Download Kinetics-400 labels if needed
# LABELS_URL = "https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt"
# urllib.request.urlretrieve(LABELS_URL, "kinetics_labels.txt")
with open("kinetics_labels.txt", "r") as f:
    KINETICS_LABELS = [line.strip().lower() for line in f.readlines()]

print(f"Loaded {len(KINETICS_LABELS)} Kinetics-400 labels")

from pytorchvideo.transforms import Normalize, UniformTemporalSubsample, ShortSideScale, ApplyTransformToKey
from torchvision.transforms import Compose, Lambda, CenterCrop

# Define model input transform pipeline (matching SlowFast pretrained)
transform = ApplyTransformToKey(
    key="video",
    transform=Compose([
        UniformTemporalSubsample(32),
        Lambda(lambda x: x / 255.0),
        Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]),
        ShortSideScale(256),
        CenterCrop(224)
    ]),
)

# SlowFast needs two pathway inputs
def pack_pathway(frames):
    fast_pathway = frames
    slow_pathway = torch.index_select(
        frames, 2, torch.linspace(0, frames.shape[2] - 1, frames.shape[2] // 4).long().to(frames.device)
    )
    return [slow_pathway, fast_pathway]

# Load SlowFast pretrained model
action_model = slowfast_r50(pretrained=True).to(DEVICE)
action_model.eval()

# Define binary punch vs non-punch keywords
PUNCH_KEYWORDS = ["punch", "beatboxing"
,"punching bag"
,"punching person (boxing)"]

# Helper to predict video clip
def predict_clip(video_path, start_sec, end_sec):
    video = EncodedVideo.from_path(video_path)
    clip = video.get_clip(start_sec=start_sec, end_sec=end_sec)
    clip = transform(clip)
    inputs = pack_pathway(clip["video"].unsqueeze(0).to(DEVICE))  # Add batch dim

    with torch.no_grad():
        preds = action_model(inputs)
        preds = torch.nn.functional.softmax(preds, dim=1)

    conf, pred_class = torch.max(preds, 1)
    label = KINETICS_LABELS[pred_class.item()]
    is_punch = 1 if any(k in label for k in PUNCH_KEYWORDS) else 0
    return is_punch, conf.item(), label

def evaluate_videos_in_folder(folder_path):
    video_preds = []
    video_confs = []

    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.avi','.mp4','.mov'))]

    for video_path in files:
        try:
            video = EncodedVideo.from_path(video_path)
            duration = video.duration

            clips_preds = []
            clips_confs = []

            clip_duration = 3  # seconds per clip
            start_sec = 0

            while start_sec + clip_duration <= duration:
                pred, conf, _ = predict_clip(video_path, start_sec, start_sec + clip_duration)
                clips_preds.append(pred)
                clips_confs.append(conf)
                start_sec += clip_duration

            final_pred = int(np.round(np.mean(clips_preds)))
            avg_conf = float(np.mean(clips_confs))

            video_preds.append(final_pred)
            video_confs.append(avg_conf)
        except Exception as e:
            print(f"Error processing {video_path}: {e}")
            continue

    return video_preds, video_confs

# Main evaluation loop per class
def evaluate_dataset(data_dir):
    classes = ['non_punch', 'punch']
    label_to_idx = {cls: idx for idx, cls in enumerate(classes)}
    all_true = []
    all_pred = []
    all_conf = []

    for cls in classes:
        folder = os.path.join(data_dir, cls)
        print(f"Evaluating class '{cls}' in folder {folder}")
        video_preds, video_confs = evaluate_videos_in_folder(folder)
        all_true += [label_to_idx[cls]] * len(video_preds)
        all_pred += video_preds
        all_conf += video_confs

    return np.array(all_true), np.array(all_pred), np.array(all_conf)

# Run evaluation
test_dir = "/content/drive/MyDrive/action_data"  # Update your path
true_labels, preds, confidences = evaluate_dataset(test_dir)

# Calculate metrics
accuracy = (preds == true_labels).mean()
precision = precision_score(true_labels, preds)
recall = recall_score(true_labels, preds)
f1 = f1_score(true_labels, preds)
ap = average_precision_score(true_labels, confidences)
cm = confusion_matrix(true_labels, preds)
report = classification_report(true_labels, preds, target_names=["non_punch", "punch"])

print("\nModel Evaluation Metrics")
print("-" * 40)
print(f"Accuracy           : {accuracy:.4f}")
print(f"Precision          : {precision:.4f}")
print(f"Recall             : {recall:.4f}")
print(f"F1 Score           : {f1:.4f}")
print(f"Average Precision  : {ap:.4f}")
print("Confusion Matrix:\n", cm)
print("Classification Report:\n", report)

# Plot confusion matrix
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["non_punch", "punch"]).plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

# Plot precision-recall curve
from sklearn.metrics import PrecisionRecallDisplay
PrecisionRecallDisplay.from_predictions(true_labels, confidences)
plt.title("Precision-Recall Curve")
plt.show()

